{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import sqlparse\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHashEmbeddings(Embeddings):\n",
    "    \"\"\"Simple deterministic hash-based embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int = 384):\n",
    "        self.dimension = dimension\n",
    "    \n",
    "    def _hash_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Convert text to a deterministic vector using hash\"\"\"\n",
    "        hash_object = hashlib.sha256(text.encode())\n",
    "        hash_hex = hash_object.hexdigest()\n",
    "        \n",
    "        float_array = []\n",
    "        for i in range(0, len(hash_hex), 8):\n",
    "            chunk = hash_hex[i:i+8]\n",
    "            float_val = int(chunk, 16) / 2**32 - 1\n",
    "            float_array.append(float_val)\n",
    "        \n",
    "        array = np.array(float_array, dtype=np.float32)\n",
    "        \n",
    "        if len(array) < self.dimension:\n",
    "            array = np.pad(array, (0, self.dimension - len(array)))\n",
    "        else:\n",
    "            array = array[:self.dimension]\n",
    "            \n",
    "        norm = np.linalg.norm(array)\n",
    "        if norm > 0:\n",
    "            array = array / norm\n",
    "            \n",
    "        return array.tolist()\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of texts\"\"\"\n",
    "        return [self._hash_text(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single text\"\"\"\n",
    "        return self._hash_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessPackageManager:\n",
    "    \"\"\"Specialized manager for chess engine packages\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=\"documents\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.packages_path = self.base_path / \"packages\"\n",
    "        self.metadata_path = self.base_path / \"metadata\"\n",
    "        \n",
    "    def validate_paths(self):\n",
    "        \"\"\"Validate that required paths exist\"\"\"\n",
    "        if not self.packages_path.exists():\n",
    "            raise ValueError(f\"Packages directory not found: {self.packages_path}\")\n",
    "        if not self.metadata_path.exists():\n",
    "            print(f\"Creating metadata directory: {self.metadata_path}\")\n",
    "            self.metadata_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load chess package metadata\"\"\"\n",
    "        metadata_file = self.metadata_path / \"package_info.json\"\n",
    "        if metadata_file.exists():\n",
    "            try:\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading metadata file: {e}. Using empty metadata.\")\n",
    "                return {}\n",
    "        return {}\n",
    "\n",
    "    def get_package_dependencies(self):\n",
    "        \"\"\"Extract package dependencies from metadata\"\"\"\n",
    "        metadata = self.load_metadata()\n",
    "        dependencies = {}\n",
    "        if 'chess' in metadata and 'packages' in metadata['chess']:\n",
    "            for pkg, info in metadata['chess']['packages'].items():\n",
    "                dependencies[pkg] = info.get('dependencies', [])\n",
    "        return dependencies\n",
    "\n",
    "    def get_package_details(self):\n",
    "        \"\"\"Get detailed information about each package\"\"\"\n",
    "        metadata = self.load_metadata()\n",
    "        if 'chess' in metadata and 'packages' in metadata['chess']:\n",
    "            return metadata['chess']['packages']\n",
    "        return {}\n",
    "\n",
    "    def get_plsql_files(self):\n",
    "        \"\"\"Get all PL/SQL files in the packages directory\"\"\"\n",
    "        return list(self.packages_path.glob(\"*.pk[sb]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSQLParser:\n",
    "    \"\"\"Enhanced parser for PL/SQL chess packages\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_package(content):\n",
    "        \"\"\"Parse package content and extract chess-specific metadata\"\"\"\n",
    "        content = content.strip()\n",
    "        \n",
    "        package_match = re.search(r'package\\s+(\\w+)', content, re.IGNORECASE)\n",
    "        package_name = package_match.group(1) if package_match else \"Unknown\"\n",
    "        \n",
    "        routines = []\n",
    "        procedure_matches = re.finditer(\n",
    "            r'(procedure|function)\\s+(\\w+)[^;]*?(\\(.*?\\))?\\s*(return\\s+\\w+)?',\n",
    "            content,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        for match in procedure_matches:\n",
    "            routine_type = match.group(1)\n",
    "            routine_name = match.group(2)\n",
    "            parameters = match.group(3) or \"\"\n",
    "            return_type = match.group(4) or \"\"\n",
    "            \n",
    "            category = \"general\"\n",
    "            if any(term in routine_name.lower() for term in ['move', 'position', 'piece']):\n",
    "                category = \"move_generation\"\n",
    "            elif any(term in routine_name.lower() for term in ['eval', 'score', 'value']):\n",
    "                category = \"evaluation\"\n",
    "            elif any(term in routine_name.lower() for term in ['fen', 'pgn', 'notation']):\n",
    "                category = \"notation\"\n",
    "            \n",
    "            routines.append({\n",
    "                \"type\": routine_type,\n",
    "                \"name\": routine_name,\n",
    "                \"parameters\": parameters.strip(\"()\"),\n",
    "                \"return_type\": return_type.replace(\"return \", \"\") if return_type else None,\n",
    "                \"category\": category\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"package_name\": package_name,\n",
    "            \"routines\": routines,\n",
    "            \"content\": content\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_documents():\n",
    "    \"\"\"Load and process all documents\"\"\"\n",
    "    chess_manager = ChessPackageManager()\n",
    "    chess_manager.validate_paths()\n",
    "    \n",
    "    documents = []\n",
    "    metadata = []\n",
    "    \n",
    "    package_details = chess_manager.get_package_details()\n",
    "    dependencies = chess_manager.get_package_dependencies()\n",
    "    plsql_files = chess_manager.get_plsql_files()\n",
    "    \n",
    "    print(f\"Found {len(plsql_files)} PL/SQL files\")\n",
    "    \n",
    "    for file_path in plsql_files:\n",
    "        print(f\"Processing {file_path.name}...\")\n",
    "        \n",
    "        # Try different encodings\n",
    "        encodings = ['latin1', 'cp1252', 'iso-8859-1', 'utf-8']\n",
    "        content = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as f:\n",
    "                    content = f.read()\n",
    "                    print(f\"Successfully read with {encoding} encoding\")\n",
    "                    break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if content is None:\n",
    "            print(f\"Could not read {file_path.name} with any supported encoding\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parsed = PLSQLParser.parse_package(content)\n",
    "            package_name = parsed[\"package_name\"]\n",
    "            \n",
    "            if package_name.lower() in package_details:\n",
    "                parsed.update(package_details[package_name.lower()])\n",
    "            \n",
    "            parsed[\"dependencies\"] = dependencies.get(package_name.lower(), [])\n",
    "            \n",
    "            formatted_content = sqlparse.format(\n",
    "                content,\n",
    "                reindent=True,\n",
    "                keyword_case='upper'\n",
    "            )\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"package_name\": parsed[\"package_name\"],\n",
    "                    \"routines\": parsed[\"routines\"],\n",
    "                    \"purpose\": parsed.get(\"purpose\", \"\"),\n",
    "                    \"dependencies\": parsed[\"dependencies\"],\n",
    "                    \"file_type\": file_path.suffix,\n",
    "                    \"formatted_content\": formatted_content,\n",
    "                    \"source\": str(file_path),\n",
    "                    \"file_name\": file_path.name\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            metadata.append(parsed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} PL/SQL packages\")\n",
    "    return documents, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def create_vectorstore(documents):\n",
    "    \"\"\"Create and save the vector store\"\"\"\n",
    "    print(\"Starting document processing...\")\n",
    "    print(f\"Number of documents to process: {len(documents)}\")\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    print(\"Initializing text splitter...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split documents\n",
    "    print(\"Splitting documents...\")\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(splits)} splits from the documents\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    print(\"Initializing embeddings...\")\n",
    "    embeddings = SimpleHashEmbeddings(dimension=384)\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"Creating FAISS vector store...\")\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "    # # Save vector store\n",
    "    # print(\"Saving vector store...\")\n",
    "    # vectorstore.save_local(\"vectorstore\")\n",
    "    \n",
    "    # # Save metadata\n",
    "    # print(\"Saving metadata...\")\n",
    "    # with open(\"chess_metadata.json\", \"w\") as f:\n",
    "    #     json.dump([doc.metadata for doc in documents], f, indent=2)\n",
    "\n",
    "    # In your document_preprocessor.ipynb\n",
    "    vectorstore.save_local(\"../data/vectorstore\")\n",
    "\n",
    "    # Save metadata\n",
    "    with open(\"../data/metadata/chess_metadata.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"packages\": metadata,\n",
    "            \"total_documents\": len(documents),\n",
    "            \"creation_date\": datetime.datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing pipeline...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting document processing pipeline...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 PL/SQL files\n",
      "Processing pl_pig_chess_interface.pkb...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_engine.pks...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_engine_eval.pkb...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_data.pks...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_engine.pkb...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_interface.pks...\n",
      "Successfully read with latin1 encoding\n",
      "Processing pl_pig_chess_engine_eval.pks...\n",
      "Successfully read with latin1 encoding\n",
      "Successfully loaded 7 PL/SQL packages\n",
      "\n",
      "Document Loading Complete!\n"
     ]
    }
   ],
   "source": [
    "documents, metadata = load_and_process_documents()\n",
    "print(\"\\nDocument Loading Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing...\n",
      "Number of documents to process: 7\n",
      "Initializing text splitter...\n",
      "Splitting documents...\n",
      "Created 1665 splits from the documents\n",
      "Initializing embeddings...\n",
      "Creating FAISS vector store...\n",
      "Processing complete!\n",
      "\n",
      "Vector Store Creation Complete!\n"
     ]
    }
   ],
   "source": [
    "vectorstore = create_vectorstore(documents)\n",
    "print(\"\\nVector Store Creation Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the metadata separately for easy access\n",
    "import datetime\n",
    "\n",
    "\n",
    "with open(\"chess_metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"packages\": metadata,\n",
    "        \"total_documents\": len(documents),\n",
    "        \"creation_date\": datetime.datetime.now().isoformat()\n",
    "    }, f, indent=2)\n",
    "print(\"\\nMetadata Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector Store Loading Test Complete!\n"
     ]
    }
   ],
   "source": [
    "# Test the vector store\n",
    "test_embeddings = SimpleHashEmbeddings(dimension=384)\n",
    "loaded_vectorstore = FAISS.load_local(\"vectorstore\", test_embeddings)\n",
    "print(\"\\nVector Store Loading Test Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query Results:\n",
      "\n",
      "Result 1:\n",
      "Package: BODY\n",
      "File: pl_pig_chess_engine_eval.pkb\n",
      "\n",
      "Result 2:\n",
      "Package: PL_PIG_CHESS_DATA\n",
      "File: pl_pig_chess_data.pks\n",
      "\n",
      "Processing Pipeline Complete!\n"
     ]
    }
   ],
   "source": [
    "# Test a simple query\n",
    "test_query = \"How does move generation work?\"\n",
    "docs = loaded_vectorstore.similarity_search(test_query, k=2)\n",
    "print(\"\\nTest Query Results:\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Package: {doc.metadata['package_name']}\")\n",
    "    print(f\"File: {doc.metadata['file_name']}\")\n",
    "\n",
    "print(\"\\nProcessing Pipeline Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
